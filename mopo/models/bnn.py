from __future__ import division
from __future__ import print_function
from __future__ import absolute_import

import os
import time
import pdb
import itertools
from collections import OrderedDict

import tensorflow as tf
import numpy as np
from tqdm import trange
from scipy.io import savemat, loadmat

from mopo.models.utils import get_required_argument, TensorStandardScaler
from mopo.models.fc import FC

from mopo.utils.logging import Progress, Silent, Wandb


np.set_printoptions(precision=5)


class BNN:
    """Neural network models which model aleatoric uncertainty (and possibly epistemic uncertainty
    with ensembling).
    """
    def __init__(self, params):
        """Initializes a class instance.

        Arguments:
            params (DotMap): A dotmap of model parameters.
                .name (str): Model name, used for logging/use in variable scopes.
                    Warning: Models with the same name will overwrite each other.
                .num_networks (int): (optional) The number of networks in the ensemble. Defaults to 1.
                    Ignored if model is being loaded.
                .model_dir (str/None): (optional) Path to directory from which model will be loaded, and
                    saved by default. Defaults to None.
                .load_model (bool): (optional) If True, model will be loaded from the model directory,
                    assuming that the files are generated by a model of the same name. Defaults to False.
                .sess (tf.Session/None): The session that this model will use.
                    If None, creates a session with its own associated graph. Defaults to None.
                .rex (bool): (optional) If True, we add the V-REx penalty to the loss during dynamics
                    training.
                .rex_beta (float): (optional) The penalty value to use in V-REx.
                .rex_multiply (`bool`): If True, multiply variance by beta, else divide sum of losses
                    by beta.
                .rex_std (`bool`): If True, use std instead of var in REx.
                .lr_decay ('float'): (optional) Multiply the core loss by this number before returning.
                    Applies in REx training loop.
                .log_dir (str): Where to save logs to during training.
        """
        print('params', params)
        self.name = get_required_argument(params, 'name', 'Must provide name.')
        self.model_dir = params.get('model_dir', None)
        self._log_dir = params.get('log_dir', None)

        self.train_bnn_only = params.get('train_bnn_only', None)
        if self.train_bnn_only:
            self.domain = self._log_dir.split('/')[-3]
            self.exp_seed = self._log_dir.split('/')[-1].split('_')[0]
            self.exp_name = self._log_dir.split('/')[-2] + '_' + self.exp_seed
            print('self.exp_name', self.exp_name)
            print("'_'+self.domain+'_bnn'", '_'+self.domain+'_bnn')
            self.wlogger = Wandb(params, name=self.exp_name, project='_'+self.domain+'_bnn')

        print('[ BNN ] Initializing model: {} | {} networks | {} elites'.format(params['name'], params['num_networks'], params['num_elites']))
        if params.get('sess', None) is None:
            config = tf.ConfigProto()
            # config.gpu_options.allow_growth = True
            self._sess = tf.Session(config=config)
        else:
            self._sess = params.get('sess')

        # Instance variables
        self.finalized = False
        self.layers, self.max_logvar, self.min_logvar = [], None, None
        self.decays, self.optvars, self.nonoptvars = [], [], []
        self.end_act, self.end_act_name = None, None
        self.scaler = None
        
        # REx parameters
        self.rex = params.get('rex', False)
        self.rex_beta = float(params.get('rex_beta', 10.0))
        self.rex_multiply = params.get('rex_multiply', False)
        self.lr_decay = float(params.get('lr_decay', 1.0))

        # Training objects
        self.optimizer = None
        self.sy_train_in, self.sy_train_targ, self.sy_train_pol, self.sy_rex_training_loop = None, None, None, None
        self.train_op, self.mse_loss, self.mse_pol_tot_loss, self.mse_pol_var_loss, self.mse_mean_pol_loss = None, None, None, None, None

        # Prediction objects
        self.sy_pred_in2d, self.sy_pred_mean2d_fac, self.sy_pred_var2d_fac = None, None, None
        self.sy_pred_mean2d, self.sy_pred_var2d = None, None
        self.sy_pred_in3d, self.sy_pred_mean3d_fac, self.sy_pred_var3d_fac = None, None, None

        self.deterministic = params.get('deterministic', False)
        self.separate_mean_var = params.get('separate_mean_var', False)

        if params.get('load_model', False):
            if self.model_dir is None:
                raise ValueError("Cannot load model without providing model directory.")

            self._load_structure()
            self.num_nets, self.model_loaded = self.layers[0].get_ensemble_size(), True

            print("Model loaded from %s." % self.model_dir)
            self.num_elites = params['num_elites']
        else:
            self.num_nets = params.get('num_networks', 1)
            self.num_elites = params['num_elites'] #params.get('num_elites', 1)
            self.model_loaded = False

            # Corrected the indentation on the below code block
            # Previous version would wipe the `self.var_layers` property of the loaded model
            if self.separate_mean_var:
                self.var_layers = []
            else:
                self.var_layers = None

            if self.num_nets == 1:
                print("Created a neural network with variance predictions.")
            else:
                print("Created an ensemble of {} neural networks with variance predictions | Elites: {}".format(self.num_nets, self.num_elites))

    @property
    def is_probabilistic(self):
        return True

    @property
    def is_tf_model(self):
        return True

    @property
    def sess(self):
        return self._sess

    ###################################
    # Network Structure Setup Methods #
    ###################################

    def add(self, layer, var_layer=False):
        """Adds a new layer to the network.

        Arguments:
            layer: (layer) The new layer to be added to the network.
                   If this is the first layer, the input dimension of the layer must be set.

        Returns: None.
        """
        if self.finalized:
            raise RuntimeError("Cannot modify network structure after finalizing.")
        if not var_layer:
            if len(self.layers) == 0 and layer.get_input_dim() is None:
                raise ValueError("Must set input dimension for the first layer.")
        else:
            if len(self.var_layers) == 0 and layer.get_input_dim() is None:
                raise ValueError("Must set input dimension for the first layer.")
        if self.model_loaded:
            raise RuntimeError("Cannot add layers to a loaded model.")

        layer.set_ensemble_size(self.num_nets)
        if not var_layer:
            if len(self.layers) > 0:
                layer.set_input_dim(self.layers[-1].get_output_dim())
            self.layers.append(layer.copy())
        else:
            if len(self.var_layers) > 0:
                layer.set_input_dim(self.var_layers[-1].get_output_dim())
            self.var_layers.append(layer.copy())
        print('Added layer with input dim', layer.get_input_dim(), ', output dim', layer.get_output_dim())

    def pop(self, var_layer=False):
        """Removes and returns the most recently added layer to the network.

        Returns: (layer) The removed layer.
        """
        if not var_layer:
            if len(self.layers) == 0:
                raise RuntimeError("Network is empty.")
        else:
            if len(self.var_layers) == 0:
                raise RuntimeError("Network is empty.")
        if self.finalized:
            raise RuntimeError("Cannot modify network structure after finalizing.")
        if self.model_loaded:
            raise RuntimeError("Cannot remove layers from a loaded model.")
        if var_layer:
            return self.var_layers.pop()
        return self.layers.pop()

    def finalize(self, optimizer, optimizer_args=None, *args, **kwargs):
        """Finalizes the network.

        Arguments:
            optimizer: (tf.train.Optimizer) An optimizer class from those available at tf.train.Optimizer.
            optimizer_args: (dict) A dictionary of arguments for the __init__ method of the chosen optimizer.

        Returns: None
        """
        if len(self.layers) == 0 or (self.separate_mean_var and len(self.var_layers) == 0):
            raise RuntimeError("Cannot finalize an empty network.")
        if self.finalized:
            raise RuntimeError("Can only finalize a network once.")

        optimizer_args = {} if optimizer_args is None else optimizer_args
        self.optimizer = optimizer(**optimizer_args)

        if not self.separate_mean_var:
            # Add variance output.
            self.layers[-1].set_output_dim(2 * self.layers[-1].get_output_dim())

            # Remove last activation to isolate variance from activation function.
            self.end_act = self.layers[-1].get_activation()
            self.end_act_name = self.layers[-1].get_activation(as_func=False)
            self.layers[-1].unset_activation()

            # Construct all variables.
            with self.sess.as_default():
                with tf.variable_scope(self.name):
                    self.scaler = TensorStandardScaler(self.layers[0].get_input_dim())
                    self.max_logvar = tf.Variable(np.ones([1, self.layers[-1].get_output_dim() // 2])/2., dtype=tf.float32,
                                                  name="max_log_var")
                    self.min_logvar = tf.Variable(-np.ones([1, self.layers[-1].get_output_dim() // 2])*10., dtype=tf.float32,
                                                  name="min_log_var")
                    for i, layer in enumerate(self.layers):
                        with tf.variable_scope("Layer%i" % i):
                            layer.construct_vars()
                            self.decays.extend(layer.get_decays())
                            self.optvars.extend(layer.get_vars())
        else:
            # Construct all variables.
            with self.sess.as_default():
                with tf.variable_scope(self.name):
                    self.scaler = TensorStandardScaler(self.layers[0].get_input_dim())
                    self.max_logvar = tf.Variable(np.ones([1, self.var_layers[-1].get_output_dim()])/2., dtype=tf.float32,
                                                  name="max_log_var")
                    self.min_logvar = tf.Variable(-np.ones([1, self.var_layers[-1].get_output_dim()])*10., dtype=tf.float32,
                                                  name="min_log_var")
                    for i, layer in enumerate(self.layers):
                        with tf.variable_scope("Layer%i_mean" % i):
                            layer.construct_vars()
                            self.decays.extend(layer.get_decays())
                            self.optvars.extend(layer.get_vars())
                    for i, layer in enumerate(self.var_layers):
                        with tf.variable_scope("Layer%i_var" % i):
                            layer.construct_vars()
                            self.decays.extend(layer.get_decays())
                            self.optvars.extend(layer.get_vars())
        self.optvars.extend([self.max_logvar, self.min_logvar])
        self.nonoptvars.extend(self.scaler.get_vars())

        # Set up training
        with tf.variable_scope(self.name):
            self.optimizer = optimizer(**optimizer_args)
            self.sy_train_in = tf.placeholder(dtype=tf.float32,
                                              shape=[self.num_nets, None, self.layers[0].get_input_dim()],
                                              name="training_inputs")
            if not self.separate_mean_var:
                self.sy_train_targ = tf.placeholder(dtype=tf.float32,
                                                    shape=[self.num_nets, None, self.layers[-1].get_output_dim() // 2],
                                                    name="training_targets")
            else:
                self.sy_train_targ = tf.placeholder(dtype=tf.float32,
                                                    shape=[self.num_nets, None, self.layers[-1].get_output_dim()],
                                                    name="training_targets")
            self.sy_train_pol = tf.placeholder(dtype=tf.float32,
                                              shape=[self.num_nets, None, 1],
                                              name="training_policies")
            self.sy_rex_training_loop = tf.placeholder(dtype=bool,
                                                    shape=(),
                                                    name="training_rex_loop")

            print('self.deterministic', self.deterministic)
            if not self.deterministic:
                self.train_core_losses, self.train_pol_tot_loss, self.train_pol_var_loss, self.train_mean_pol_loss = self._compile_losses(self.sy_train_in, self.sy_train_targ, self.sy_train_pol, rex_training_loop=self.sy_rex_training_loop, inc_var_loss=True)
                self.train_core_loss = tf.reduce_sum(self.train_core_losses)
                print('self.train_core_loss', self.train_core_loss)
                self.train_decay_loss = tf.add_n(self.decays)
                self.train_var_lim_loss = 0.01 * tf.reduce_sum(self.max_logvar) - 0.01 * tf.reduce_sum(self.min_logvar)
                self.train_loss = self.train_core_loss + self.train_decay_loss + self.train_var_lim_loss
            else:
                self.train_core_loss, _, _, _ = self._compile_losses(self.sy_train_in, self.sy_train_targ, self.sy_train_pol, rex_training_loop=self.sy_rex_training_loop, inc_var_loss=False)
                self.train_decay_loss = tf.add_n(self.decays)
                self.train_var_lim_loss = None  # Note: will error when attempt is made to save to disk - but this logic branch should not be invoked
                self.train_loss = self.train_core_loss + self.train_decay_loss

            self.mse_loss, self.mse_pol_tot_loss, self.mse_pol_var_loss, self.mse_mean_pol_loss = self._compile_losses(self.sy_train_in, self.sy_train_targ, self.sy_train_pol, rex_training_loop=self.sy_rex_training_loop, inc_var_loss=False)
            
            # If we are in the REx training loop, multiply the training loss by the learning rate decay
            # We do not make any modifications to the original MOPO training procedure in the initial training loop
            self.train_loss = tf.cond(self.sy_rex_training_loop,
                lambda: self.lr_decay * self.train_loss,
                lambda: self.train_loss
            )
            
            self.train_op = self.optimizer.minimize(self.train_loss, var_list=self.optvars)

        # Initialize all variables
        self.sess.run(tf.variables_initializer(self.optvars + self.nonoptvars + self.optimizer.variables()))

        # Set up prediction
        with tf.variable_scope(self.name):
            self.sy_pred_in2d = tf.placeholder(dtype=tf.float32,
                                               shape=[None, self.layers[0].get_input_dim()],
                                               name="2D_training_inputs")
            self.sy_pred_mean2d_fac, self.sy_pred_var2d_fac = \
                self.create_prediction_tensors(self.sy_pred_in2d, factored=True)
            self.sy_pred_mean2d = tf.reduce_mean(self.sy_pred_mean2d_fac, axis=0)
            self.sy_pred_var2d = tf.reduce_mean(self.sy_pred_var2d_fac, axis=0) + \
                tf.reduce_mean(tf.square(self.sy_pred_mean2d_fac - self.sy_pred_mean2d), axis=0)

            self.sy_pred_in3d = tf.placeholder(dtype=tf.float32,
                                               shape=[self.num_nets, None, self.layers[0].get_input_dim()],
                                               name="3D_training_inputs")
            self.sy_pred_mean3d_fac, self.sy_pred_var3d_fac = \
                self.create_prediction_tensors(self.sy_pred_in3d, factored=True)

        # Load model if needed
        if self.model_loaded:
            self.load_params()
        self.finalized = True

    def load_params(self):
        with self.sess.as_default():
            params_dict = loadmat(os.path.join(self.model_dir, "%s_0.mat" % self.name))
            all_vars = self.nonoptvars + self.optvars
            for i, var in enumerate(all_vars):
                var.load(params_dict[str(i)])

    ##################
    # Custom Methods #
    ##################

    def _save_state(self, idx):
        self._state[idx] = [layer.get_model_vars(idx, self.sess) for layer in self.layers]
        if self.separate_mean_var:
            self._state[idx].extend([layer.get_model_vars(idx, self.sess) for layer in self.var_layers])

    def _set_state(self):
        keys = ['weights', 'biases']
        ops = []
        num_layers = len(self.layers)
        for layer in range(num_layers):
            # net_state = self._state[i]
            params = {key: np.stack([self._state[net][layer][key] for net in range(self.num_nets)]) for key in keys}
            ops.extend(self.layers[layer].set_model_vars(params))
        if self.separate_mean_var:
            num_layers = len(self.var_layers)
            for layer in range(num_layers):
                # net_state = self._state[i]
                params = {key: np.stack([self._state[net][len(self.layers)+layer][key] for net in range(self.num_nets)]) for key in keys}
                ops.extend(self.var_layers[layer].set_model_vars(params))
        self.sess.run(ops)

    def _save_best(self, epoch, holdout_losses):
        updated = False
        for i in range(len(holdout_losses)):
            current = holdout_losses[i]
            _, best = self._snapshots[i]
            improvement = (best - current) / np.abs(best)
            if improvement > 0.01:
                self._snapshots[i] = (epoch, current)
                self._save_state(i)
                updated = True
                improvement = (best - current) / np.abs(best)
                print('epoch {} | updated {} | improvement: {:.4f} | best: {:.4f} | current: {:.4f}'.format(epoch, i, improvement, best, current))
        
        if updated:
            self._epochs_since_update = 0
        else:
            self._epochs_since_update += 1

        if self._epochs_since_update > self._max_epochs_since_update:
            print('[ BNN ] Breaking at epoch {}: {} epochs since update ({} max)'.format(epoch, self._epochs_since_update, self._max_epochs_since_update))
            return True
        else:
            return False

    def _start_train(self):
        self._state = {}
        self._snapshots = {i: (None, 1e10) for i in range(self.num_nets)}
        self._epochs_since_update = 0

    def _end_train(self, holdout_losses):
        sorted_inds = np.argsort(holdout_losses)
        self._model_inds = sorted_inds[:self.num_elites].tolist()
        print('Using {} / {} models: {}'.format(self.num_elites, self.num_nets, self._model_inds))

    def random_inds(self, batch_size):
        inds = np.random.choice(self._model_inds, size=batch_size)
        return inds

    def reset(self):
        print('[ BNN ] Resetting model')
        [layer.reset(self.sess) for layer in self.layers]
        if self.separate_mean_var:
            [layer.reset(self.sess) for layer in self.var_layers]

    def validate(self, inputs, targets, policies):
        # This function does not appear to be called.
        # Assumed that it is most reasonable to run as if it were in the REx training loop.
        inputs = np.tile(inputs[None], [self.num_nets, 1, 1])
        targets = np.tile(targets[None], [self.num_nets, 1, 1])
        losses = self.sess.run(
            self.mse_loss,
            feed_dict={
                self.sy_train_in: inputs,
                self.sy_train_targ: targets,
                self.sy_train_pol: policies,
                self.sy_rex_training_loop: True,
                }
        )
        mean_elite_loss = np.sort(losses)[:self.num_elites].mean()
        return mean_elite_loss

    def _save_training_losses(self, train_loss, train_core_loss, train_pol_tot_loss, train_pol_var_loss, train_mean_pol_loss, train_decay_loss, train_var_lim_loss, n_datapoints, n_baches, epoch, rex_training_loop):
        """Save the current training losses.
        """
        prefix = 'train/'
        print('train_pol_tot_loss', train_pol_tot_loss.shape)
        print('train_pol_var_loss', train_pol_var_loss.shape)
        print('train_mean_pol_loss', train_mean_pol_loss.shape)

        self.wlogger.wandb.log({**{'train/loss': train_loss,
                                   'train/core_loss': train_core_loss,
                                   'train/decay_loss': train_decay_loss,
                                   'train/rex_training_loop': int(rex_training_loop),
                                   f'train/var_lim_loss': train_var_lim_loss,
                                   'train/n_datapoints': n_datapoints, 'train/n_baches': n_baches, 'train/epoch': epoch,
                                   prefix + 'pol_total_losses_mean': np.mean(train_pol_tot_loss),
                                   prefix + 'pol_var_losses_mean': np.mean(train_pol_var_loss),
                                   prefix + 'mean_pol_losses_mean': np.mean(train_mean_pol_loss),
                                   prefix + 'pol_total_losses_var': np.var(train_pol_tot_loss),
                                   prefix + 'pol_var_losses_var': np.var(train_pol_var_loss),
                                   prefix + 'mean_pol_losses_var': np.var(train_mean_pol_loss),
                                   },
                               **{f'train/M{i}_pol_tot_loss': train_pol_tot_loss[i] for i in range(len(train_pol_tot_loss))},
                               **{f'train/M{i}_pol_var_loss': train_pol_var_loss[i] for i in range(len(train_pol_var_loss))},
                               **{f'train/M{i}_mean_pol_loss': train_mean_pol_loss[i] for i in range(len(train_mean_pol_loss))},
                                }, step=n_baches)

    def _save_losses(self, total_losses, pol_total_losses, pol_var_losses, mean_pol_losses, n_datapoints, n_baches, epoch, holdout=False):
        """Save the current training/holdout evaluation losses.
        """
        prefix = 'holdout/' if holdout else ''

        d = {**{'n_datapoints': n_datapoints, 'n_baches': n_baches, 'epoch': epoch,
                prefix + 'total_losses_mean': np.mean(total_losses),
                prefix + 'pol_total_losses_mean': np.mean(pol_total_losses),
                prefix + 'pol_var_losses_mean': np.mean(pol_var_losses),
                prefix + 'mean_pol_losses_mean': np.mean(mean_pol_losses),
                prefix + 'total_losses_var': np.var(total_losses),
                prefix + 'pol_total_losses_var': np.var(pol_total_losses),
                prefix + 'pol_var_losses_var': np.var(pol_var_losses),
                prefix + 'mean_pol_losses_var': np.var(mean_pol_losses),
                },
             **{prefix + f'M{i}_total_losses': total_losses[i] for i in range(len(total_losses))},
             **{prefix + f'M{i}_pol_total_losses': pol_total_losses[i] for i in range(len(pol_total_losses))},
             **{prefix + f'M{i}_pol_var_losses': pol_var_losses[i] for i in range(len(pol_var_losses))},
             **{prefix + f'M{i}_mean_pol_losses': mean_pol_losses[i] for i in range(len(mean_pol_losses))}}
        # print('d', d)
        self.wlogger.wandb.log(d)



    #################
    # Model Methods #
    #################

    def train(self, inputs, targets, policies,
              batch_size=32, max_epochs=None, max_epochs_since_update=5,
              hide_progress=False, holdout_ratio=0.0, max_logging=1000, max_grad_updates=None, timer=None, max_t=None,
              holdout_policy=None, repeat_dynamics_epochs=1):
        """Trains/Continues network training

        Arguments:
            inputs (np.ndarray): Network inputs in the training dataset in rows.
            targets (np.ndarray): Network target outputs in the training dataset in rows corresponding
                to the rows in inputs.
            policies (np.ndarray): Policies in the training dataset in rows corresponding
                to the rows in inputs.
            batch_size (int): The minibatch size to be used for training.
            epochs (int): Number of epochs (full network passes that will be done.
            hide_progress (bool): If True, hides the progress bar shown at the beginning of training.
            holdout_policy (float or None): The policy to hold-out during training and use in evaluation.
                If not specified, uses holdout_ratio to randomly select records from across all policies.
            repeat_dynamics_epochs ('int'): Number of epoch sets to be completed after the original MOPO
                termination condition is met. For example, if this is set to 2 and it takes 10 epochs for
                the original termination condition to be met, then training will continue for another 20
                epochs.

        Returns: None
        """
        print('batch_size', batch_size)

        self._max_epochs_since_update = max_epochs_since_update
        self._start_train()
        break_train = False

        def shuffle_rows(arr):
            idxs = np.argsort(np.random.uniform(size=arr.shape), axis=-1)
            return arr[np.arange(arr.shape[0])[:, None], idxs]

        # Split into training and holdout sets
        num_holdout = min(int(inputs.shape[0] * holdout_ratio), max_logging)
        permutation = np.random.permutation(inputs.shape[0])

        if holdout_policy is None:
            ## Original method
            inputs, holdout_inputs = inputs[permutation[num_holdout:]], inputs[permutation[:num_holdout]]
            targets, holdout_targets = targets[permutation[num_holdout:]], targets[permutation[:num_holdout]]
            policies, holdout_policies = policies[permutation[num_holdout:]], policies[permutation[:num_holdout]]
        else:
            # Policy based method

            # Setting holdout_ratio to 1 to avoid issues with a later check
            holdout_ratio = 1.

            inputs = inputs[permutation, :]
            targets = targets[permutation, :]
            policies = policies[permutation, :]

            holdout_mask = np.squeeze(policies.astype(int))==int(holdout_policy)
            inputs, holdout_inputs = inputs[np.squeeze(np.argwhere(~holdout_mask)), :], inputs[np.squeeze(np.argwhere(holdout_mask)), :]
            targets, holdout_targets = targets[np.squeeze(np.argwhere(~holdout_mask)), :], targets[np.squeeze(np.argwhere(holdout_mask)), :]
            policies, holdout_policies = policies[np.squeeze(np.argwhere(~holdout_mask)), :], policies[np.squeeze(np.argwhere(holdout_mask)), :]

        holdout_inputs = np.tile(holdout_inputs[None], [self.num_nets, 1, 1])
        holdout_targets = np.tile(holdout_targets[None], [self.num_nets, 1, 1])
        holdout_policies = np.tile(holdout_policies[None], [self.num_nets, 1, 1])

        print('[ BNN ] Training {} | Holdout: {}'.format(inputs.shape, holdout_inputs.shape))
        with self.sess.as_default():
            self.scaler.fit(inputs)

        idxs = np.random.randint(inputs.shape[0], size=[self.num_nets, inputs.shape[0]])
        if hide_progress:
            progress = Silent()
        else:
            progress = Progress(max_epochs)

        if max_epochs is not None:
            epoch_iter = range(max_epochs)
        else:
            epoch_iter = itertools.count()

        # else:
        #     epoch_range = trange(epochs, unit="epoch(s)", desc="Network training")

        t0 = time.time()
        grad_updates = 0

        # Complete two loops of training. First loop performs normal training
        # The second loop applies REx, if REx is enabled (else the first loop is effectively completed again)
        for o_loop in range(2):
            if o_loop == 0:
                print('[ BNN ] Begginning training')
                epoch = -1
                rex_training_loop = True #False
            elif o_loop == 1:
                # Multiply the number of epochs performed in the first training loop by `repeat_dynamics_epochs`
                # and perform this number of epochs of additional training.
                # Unless a pre-trained model has been loaded, in which case do not perform any further training.
                epoch_iter = range(0) if self.model_loaded else range(repeat_dynamics_epochs*(epoch+1))
                print('epoch_iter', epoch_iter)
                print('[ BNN ] Begginning further {} epochs of training'.format(0 if self.model_loaded else repeat_dynamics_epochs*(epoch+1)))
                rex_training_loop = True
            else:
                raise RuntimeError('Attempting to complete unexpected training loop')

            print('rex_training_loop', rex_training_loop)
            save_every = 10
            n_datapoints = 0
            n_baches = 0
            for epoch in epoch_iter:
                for batch_num in range(int(np.ceil(idxs.shape[-1] / batch_size))):
                    n_datapoints += batch_num * batch_size
                    n_baches += batch_num
                    batch_idxs = idxs[:, batch_num * batch_size:(batch_num + 1) * batch_size]
                    _, train_loss, train_core_loss, train_pol_tot_loss, train_pol_var_loss, train_mean_pol_loss, train_decay_loss, train_var_lim_loss = self.sess.run(
                        (self.train_op, self.train_loss, self.train_core_loss, self.train_pol_tot_loss, self.train_pol_var_loss, self.train_mean_pol_loss, self.train_decay_loss, self.train_var_lim_loss),
                        feed_dict={
                            self.sy_train_in: inputs[batch_idxs],
                            self.sy_train_targ: targets[batch_idxs],
                            self.sy_train_pol: policies[batch_idxs],
                            self.sy_rex_training_loop: rex_training_loop,
                        }
                    )
                    if batch_num % save_every ==0:
                        self._save_training_losses(train_loss, train_core_loss, train_pol_tot_loss, train_pol_var_loss, train_mean_pol_loss, train_decay_loss, train_var_lim_loss, n_datapoints, n_baches, epoch, rex_training_loop)
                    grad_updates += 1

                idxs = shuffle_rows(idxs)
                if not hide_progress:
                    if holdout_ratio < 1e-12:
                        losses, pol_total_losses, pol_var_losses, mean_pol_losses = self.sess.run(
                                (self.mse_loss, self.mse_pol_tot_loss, self.mse_pol_var_loss, self.mse_mean_pol_loss),
                                feed_dict={
                                    self.sy_train_in: inputs[idxs[:, :max_logging]],
                                    self.sy_train_targ: targets[idxs[:, :max_logging]],
                                    self.sy_train_pol: policies[idxs[:, :max_logging]],
                                    self.sy_rex_training_loop: rex_training_loop,
                                }
                            )
                        self._save_losses(losses, pol_total_losses, pol_var_losses, mean_pol_losses, n_datapoints, n_baches, epoch)
                        named_losses = [['M{}'.format(i), losses[i]] for i in range(len(losses))]
                        progress.set_description(named_losses)
                    else:
                        losses, pol_total_losses, pol_var_losses, mean_pol_losses = self.sess.run(
                                (self.mse_loss, self.mse_pol_tot_loss, self.mse_pol_var_loss, self.mse_mean_pol_loss),
                                feed_dict={
                                    self.sy_train_in: inputs[idxs[:, :max_logging]],
                                    self.sy_train_targ: targets[idxs[:, :max_logging]],
                                    self.sy_train_pol: policies[idxs[:, :max_logging]],
                                    self.sy_rex_training_loop: rex_training_loop,
                                }
                            )
                        holdout_losses, holdout_pol_total_losses, holdout_pol_var_losses, holdout_mean_pol_losses = self.sess.run(
                                (self.mse_loss, self.mse_pol_tot_loss, self.mse_pol_var_loss, self.mse_mean_pol_loss),
                                feed_dict={
                                    self.sy_train_in: holdout_inputs,
                                    self.sy_train_targ: holdout_targets,
                                    self.sy_train_pol: holdout_policies,
                                    self.sy_rex_training_loop: rex_training_loop,
                                }
                            )
                        self._save_losses(losses, pol_total_losses, pol_var_losses, mean_pol_losses, n_datapoints, n_baches, epoch)
                        self._save_losses(holdout_losses, holdout_pol_total_losses, holdout_pol_var_losses, holdout_mean_pol_losses, n_datapoints, n_baches, epoch, holdout=True)
                        named_losses = [['M{}'.format(i), losses[i]] for i in range(len(losses))]
                        named_holdout_losses = [['V{}'.format(i), holdout_losses[i]] for i in range(len(holdout_losses))]
                        named_losses = named_losses + named_holdout_losses + [['T', time.time() - t0]]
                        progress.set_description(named_losses)

                        break_train = self._save_best(epoch, holdout_losses)
                        print('break_train', break_train)

                progress.update()
                t = time.time() - t0

                # Break conditions apply only in the first, standard training loop
                # In the second loop we force `repeat_dynamics_epochs` times the number of training epochs as in the first loop to be completed
                print('o_loop', o_loop)
                if o_loop == 0 and (break_train or (max_grad_updates and grad_updates > max_grad_updates)):
                    print('breaking the first loop')
                    break
                print('max_t', max_t)
                if max_t and t > max_t:
                    descr = 'Breaking because of timeout: {}! (max: {})'.format(t, max_t)
                    progress.append_description(descr)
                    # print('Breaking because of timeout: {}! | (max: {})\n'.format(t, max_t))
                    print('breaking the second loop')
                    # time.sleep(5)
                    break

        progress.stamp()
        if timer: timer.stamp('bnn_train')

        if not self.model_loaded:
            self._set_state()
        if timer: timer.stamp('bnn_set_state')

        holdout_losses = self.sess.run(
            self.mse_loss,
            feed_dict={
                self.sy_train_in: holdout_inputs,
                self.sy_train_targ: holdout_targets,
                self.sy_train_pol: holdout_policies,
                self.sy_rex_training_loop: rex_training_loop,
            }
        )

        if timer: timer.stamp('bnn_holdout')

        self._end_train(holdout_losses)
        if timer: timer.stamp('bnn_end')

        val_loss = (np.sort(holdout_losses)[:self.num_elites]).mean()
        model_metrics = {'val_loss': val_loss}
        print('[ BNN ] Holdout', np.sort(holdout_losses), model_metrics)
        print('finished BNN training!')
        return OrderedDict(model_metrics)
        # return np.sort(holdout_losses)[]

        # pdb.set_trace()

    def predict(self, inputs, factored=False, *args, **kwargs):
        """Returns the distribution predicted by the model for each input vector in inputs.
        Behavior is affected by the dimensionality of inputs and factored as follows:

        inputs is 2D, factored=True: Each row is treated as an input vector.
            Returns a mean of shape [ensemble_size, batch_size, output_dim] and variance of shape
            [ensemble_size, batch_size, output_dim], where N(mean[i, j, :], diag([i, j, :])) is the
            predicted output distribution by the ith model in the ensemble on input vector j.

        inputs is 2D, factored=False: Each row is treated as an input vector.
            Returns a mean of shape [batch_size, output_dim] and variance of shape
            [batch_size, output_dim], where aggregation is performed as described in the paper.

        inputs is 3D, factored=True/False: Each row in the last dimension is treated as an input vector.
            Returns a mean of shape [ensemble_size, batch_size, output_dim] and variance of sha
            [ensemble_size, batch_size, output_dim], where N(mean[i, j, :], diag([i, j, :])) is the
            predicted output distribution by the ith model in the ensemble on input vector [i, j].

        Arguments:
            inputs (np.ndarray): An array of input vectors in rows. See above for behavior.
            factored (bool): See above for behavior.
        """
        if len(inputs.shape) == 2:
            if factored:
                return self.sess.run(
                    [self.sy_pred_mean2d_fac, self.sy_pred_var2d_fac],
                    feed_dict={self.sy_pred_in2d: inputs}
                )
            else:
                return self.sess.run(
                    [self.sy_pred_mean2d, self.sy_pred_var2d],
                    feed_dict={self.sy_pred_in2d: inputs}
                )
        else:
            return self.sess.run(
                [self.sy_pred_mean3d_fac, self.sy_pred_var3d_fac],
                feed_dict={self.sy_pred_in3d: inputs}
            )

    def create_prediction_tensors(self, inputs, factored=False, *args, **kwargs):
        """See predict() above for documentation.
        """
        factored_mean, factored_variance = self._compile_outputs(inputs)
        if inputs.shape.ndims == 2 and not factored:
            mean = tf.reduce_mean(factored_mean, axis=0)
            variance = tf.reduce_mean(tf.square(factored_mean - mean), axis=0) + \
                       tf.reduce_mean(factored_variance, axis=0)
            return mean, variance
        return factored_mean, factored_variance

    def save(self, savedir, timestep):
        """Saves all information required to recreate this model in two files in savedir
        (or self.model_dir if savedir is None), one containing the model structuure and the other
        containing all variables in the network.

        savedir (str): (Optional) Path to which files will be saved. If not provided, self.model_dir
            (the directory provided at initialization) will be used.
        """
        if not self.finalized:
            raise RuntimeError()
        model_dir = self.model_dir if savedir is None else savedir

        # Write structure to file
        if not self.separate_mean_var:
            with open(os.path.join(model_dir, '{}_{}.nns'.format(self.name, timestep)), "w+") as f:
                for layer in self.layers[:-1]:
                    f.write("%s\n" % repr(layer))
                    last_layer_copy = self.layers[-1].copy()
                    last_layer_copy.set_activation(self.end_act_name)
                    last_layer_copy.set_output_dim(last_layer_copy.get_output_dim() // 2)
                    f.write("%s\n" % repr(last_layer_copy))
        else:
            with open(os.path.join(model_dir, '{}_{}.nns'.format(self.name, timestep)), "w+") as f:
                for layer in self.layers:
                    f.write("%s\n" % repr(layer))
            with open(os.path.join(model_dir, '{}_{}_var.nns'.format(self.name, timestep)), "w+") as f:
                for layer in self.var_layers:
                    f.write("%s\n" % repr(layer))

        # Save network parameters (including scalers) in a .mat file
        var_vals = {}
        for i, var_val in enumerate(self.sess.run(self.nonoptvars + self.optvars)):
            var_vals[str(i)] = var_val
        savemat(os.path.join(model_dir, '{}_{}.mat'.format(self.name, timestep)), var_vals)

    def _load_structure(self):
        """Uses the saved structure in self.model_dir with the name of this network to initialize
        the structure of this network.
        """
        structure = []
        with open(os.path.join(self.model_dir, "%s_0.nns" % self.name), "r") as f:
            for line in f:
                kwargs = {
                    key: val for (key, val) in
                    [argval.split("=") for argval in line[3:-2].split(", ")]
                }
                kwargs["input_dim"] = int(kwargs["input_dim"])
                kwargs["output_dim"] = int(kwargs["output_dim"])
                kwargs["weight_decay"] = None if kwargs["weight_decay"] == "None" else float(kwargs["weight_decay"])
                kwargs["activation"] = None if kwargs["activation"] == "None" else kwargs["activation"][1:-1]
                kwargs["ensemble_size"] = int(kwargs["ensemble_size"])
                structure.append(FC(**kwargs))
        self.layers = structure
        if self.separate_mean_var:
            var_structure = []
            with open(os.path.join(self.model_dir, "%s_0_var.nns" % self.name), "r") as f:
                for line in f:
                    kwargs = {
                        key: val for (key, val) in
                        [argval.split("=") for argval in line[3:-2].split(", ")]
                    }
                    kwargs["input_dim"] = int(kwargs["input_dim"])
                    kwargs["output_dim"] = int(kwargs["output_dim"])
                    kwargs["weight_decay"] = None if kwargs["weight_decay"] == "None" else float(kwargs["weight_decay"])
                    kwargs["activation"] = None if kwargs["activation"] == "None" else kwargs["activation"][1:-1]
                    kwargs["ensemble_size"] = int(kwargs["ensemble_size"])
                    var_structure.append(FC(**kwargs))
            self.var_layers = var_structure

    #######################
    # Compilation methods #
    #######################

    def _compile_outputs(self, inputs, ret_log_var=False):
        """Compiles the output of the network at the given inputs.

        If inputs is 2D, returns a 3D tensor where output[i] is the output of the ith network in the ensemble.
        If inputs is 3D, returns a 3D tensor where output[i] is the output of the ith network on the ith input matrix.

        Arguments:
            inputs: (tf.Tensor) A tensor representing the inputs to the network
            ret_log_var: (bool) If True, returns the log variance instead of the variance.

        Returns: (tf.Tensors) The mean and variance/log variance predictions at inputs for each network
            in the ensemble.
        """
        if not self.separate_mean_var:
            dim_output = self.layers[-1].get_output_dim()
            cur_out = self.scaler.transform(inputs)
            for layer in self.layers:
                cur_out = layer.compute_output_tensor(cur_out)

            mean = cur_out[:, :, :dim_output//2]
            if self.end_act is not None:
                mean = self.end_act(mean)

            logvar = self.max_logvar - tf.nn.softplus(self.max_logvar - cur_out[:, :, dim_output//2:])
            logvar = self.min_logvar + tf.nn.softplus(logvar - self.min_logvar)
        else:
            cur_out = self.scaler.transform(inputs)
            mean = cur_out
            means = [mean]
            # logvar = cur_out
            for layer in self.layers:
                mean = layer.compute_output_tensor(mean)
                means.append(mean)
            # assume two-head architecture
            logvar = means[-2]
            for layer in self.var_layers:
                logvar = layer.compute_output_tensor(logvar)

            logvar = self.max_logvar - tf.nn.softplus(self.max_logvar - logvar)
            logvar = self.min_logvar + tf.nn.softplus(logvar - self.min_logvar)

        if ret_log_var:
            return mean, logvar
        else:
            return mean, tf.exp(logvar)

    def _compile_losses(self, inputs, targets, policies, rex_training_loop, inc_var_loss=True):
        """Helper method for compiling the loss function.

        The loss function is obtained from the log likelihood, assuming that the output
        distribution is Gaussian, with both mean and (diagonal) covariance matrix being determined
        by network outputs.

        See the `tf_rex_loss_calculations.ipynb` notebook for more details on how REx was implemented,
        and a worked example.

        Arguments:
            inputs: (tf.Tensor) A tensor representing the input batch
            targets: (tf.Tensor) The desired targets for each input vector in inputs.
            policies: (tf.Tensor) The policy used to generate each input vector in inputs.
            rex_training_loop: (tf.Tensor) Boolean indicating whether this is the REx training loop
            inc_var_loss: (bool) If True, includes log variance loss.

        Returns: (tf.Tensor) A tensor representing the loss on the input arguments.
        """
        # These have dimensions [B N D], where:
        # B = number of networks/batches; N = number of observations; D =  number of dimensions
        mean, log_var = self._compile_outputs(inputs, ret_log_var=True)
        inv_var = tf.exp(-log_var)

        print('_compile_losses mean', mean.shape)

        # In the below the loss for each observation in each batch is determined
        # losses will have dimensions: [B N 1]; the final dimension is retained
        # as this is needed for future matrix multiplication
        if inc_var_loss:
            # Log-likelihood
            mse_losses = tf.reduce_mean(tf.square(mean - targets) * inv_var, axis=-1, keepdims=True)
            var_losses = tf.reduce_mean(log_var, axis=-1, keepdims=True)
            losses = mse_losses + var_losses
            print('_compile_losses losses', losses.shape)
        else:
            # MSE
            losses = tf.reduce_mean(tf.square(mean - targets), axis=-1, keepdims=True)

        # Identify the unique policies present across all batches of data
        policies = tf.cast(policies, tf.int32)
        unique_pols = tf.unique(tf.reshape(policies, [-1])).y

        # Create a policy one-hot encoding - this will have dimensions: [B N P], where
        # P = the number of unique policies
        pol_one_hot = tf.squeeze(tf.one_hot(policies, tf.reduce_max(unique_pols+1), axis=-1), axis=-2)
        
        # Take the sum of the observation losses for the records belonging to each policy in each batch
        # Following transposition, performing multiplication: [B P N] x [B N 1] = [B P 1]
        pol_mean_sum = tf.squeeze(tf.matmul(tf.transpose(pol_one_hot, [0,2,1]), losses), axis=-1)

        # Count the number of observations belonging to each policy in each batch
        # This also has demensions [B P 1]
        pol_count = tf.reduce_sum(pol_one_hot, axis=-2)

        # Perform a safe division - not all batches will contain a record for every policy
        # Use divide_no_nan to avoid division by 0 errors. Output dimensionality is [B P]
        policy_losses = tf.math.divide_no_nan(pol_mean_sum, pol_count)

        # Determine the mean loss for each policy across the batches
        mean_policy_losses = tf.reduce_mean(policy_losses, axis=0)

        # Add the losses across all the policies. Results in vector of length B
        policy_total_losses = tf.reduce_sum(policy_losses, axis=-1)

        # Determine the variance of the losses - use boolean mask to ensure only taking variance for
        # policies which appear in the batch (i.e., some batches may not have records for all policies).
        def determine_var(x):
            batch_pol_losses, batch_pol_counts = x[0,:], x[1,:]
            return tf.math.reduce_variance(tf.boolean_mask(batch_pol_losses, batch_pol_counts>0.))
        policy_var_losses = tf.map_fn(determine_var, tf.stack((policy_losses, pol_count), axis=-2))

        def rex_training_loop_total_losses(policy_var_losses=policy_var_losses, policy_total_losses=policy_total_losses):
            # This function is only run in the REx training loop.
            #policy_var_losses = tf.math.sqrt(policy_var_losses) #ToDo: make a flag
            if self.rex:
                if self.rex_multiply:
                    rex_tl_loss = self.rex_beta * policy_var_losses + policy_total_losses
                else:
                    rex_tl_loss = policy_var_losses + (1/self.rex_beta) * policy_total_losses
            else:
                if self.rex_multiply:
                    rex_tl_loss = policy_total_losses
                else:
                    rex_tl_loss = (1/self.rex_beta) * policy_total_losses
            return rex_tl_loss

        total_losses = tf.cond(rex_training_loop,
            rex_training_loop_total_losses,
            lambda: policy_total_losses
        )

        return total_losses, policy_total_losses, policy_var_losses, mean_policy_losses
