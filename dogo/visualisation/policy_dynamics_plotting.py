import os

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from dogo.results import get_experiment_details, PoolArrs
from dogo.pca.project import project_arr, learn_project_arr_2d
from dogo.constants import HC_STATE_DIMS, HC_ACTION_DIMS, DATA_DIR, RESULTS_BASEDIR, FIG_DIR
from dogo.visualisation.model_pool_plotting import (
    model_pool_pen_rewards_2dhist,
    model_pool_penalties_2dhist,
    model_pool_rmse_2dhist,
    model_pool_unpen_rewards_2dhist,
    model_pool_visitation_2dhist
)

class MetricNotFound(Exception):
    pass

cols = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']

def retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, env, metric):
    """ Load a metrics file, without modification. Metrics files are generated by `simulate_policy_dynamics_model`, by running episodes using a given environment model and policy.
    """
    metric_path = os.path.join(RESULTS_BASEDIR, 'analysis', 'policy', f'{dynamics_exp}_{policy_exp}_dm{deterministic_model}_dp{deterministic_policy}_{env}_{metric}.npy')
    if os.path.isfile(metric_path):
        return np.load(metric_path)
    else:
        print(f'Could not find: {metric_path}')
        raise MetricNotFound()

def retrieve_metric_stats(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, env, metric):
    """ Load a metrics file and calculate various statistics. Not all episodes will be of the same length. To obtain arrays of consistent length, the arrays of shorter episodes
    were extended to the same length as the longest episode, with np.NaN used as the filler. Hence the `np.nanX` methods are being used below.

    Episodes being of different length is common for Hopper and Walker2d environments, which have early termination conditions.
    The same is not true for the HalfCheetah environment, and so its episodes tend to always have a length of 1000 step.
    However, actions suggested by the learned policy can sometimes lead to exceptions in the MuJoCo environment, which will cut the episode short.
    """
    arr = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, env, metric)
    return {
        'mean': np.nanmean(arr, axis=-1).flatten(),
        'min': np.nanmin(arr, axis=-1).flatten(),
        'max': np.nanmax(arr, axis=-1).flatten(),
        'std': np.nanstd(arr, axis=-1).flatten()
    }

def policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy):
    dynamics_exp_details = get_experiment_details(dynamics_exp)
    policy_exp_details = get_experiment_details(policy_exp)
    policy_dynamics_exp_details = get_experiment_details(policy_exp_details.dynamics_model_exp)

    if dynamics_exp_details.rex:
        dynamics_title = f'Dynamics: {dynamics_exp_details.name} - REx: True - REx Beta: {dynamics_exp_details.rex_beta} - Seed: {dynamics_exp_details.seed} - Deterministic: {deterministic_model}'
    else:
        dynamics_title = f'Dynamics: {dynamics_exp_details.name} - REx: False - Seed: {dynamics_exp_details.seed} - Deterministic: {deterministic_model}'

    if policy_dynamics_exp_details.rex:
        policy_title = f'Policy: {policy_exp_details.name} - REx: True - REx Beta: {policy_dynamics_exp_details.rex_beta} - Seed: {policy_dynamics_exp_details.seed} - Deterministic: {deterministic_policy}'
    else:
        policy_title = f'Policy: {policy_exp_details.name} - REx: False - Seed: {policy_dynamics_exp_details.seed} - Deterministic: {deterministic_policy}'
    
    return f'{dynamics_title}\n{policy_title}'

def plot_cumulative_reward(policy_exp, dynamics_exp=None, deterministic_model=True, deterministic_policy=True):
    """ Plot the cumulative reward over the duration of a set of episodes.
    The mean and standard deviation over the set of episodes (typically 5 or 10) will be shown.
    The cumulative rewards using the learned environment and real environment will both be shown.
    The evaluation episodes use the learned dynamics, but the real reward function.
    """
    _, ax = plt.subplots(1, 1, figsize=(20,10))

    if dynamics_exp is None:
        dynamics_exp = get_experiment_details(policy_exp).dynamics_model_exp

    fake_unpen_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'fake', 'unpen_rewards')
    fake_pen = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'fake', 'reward_pens')
    eval_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'eval', 'rewards')
    gym_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'gym', 'rewards')

    n_records = fake_unpen_rewards.shape[1]

    for i, (metric, label) in enumerate([
        (fake_unpen_rewards, 'Learned Reward - No Penalty'),
        (eval_rewards, 'True Reward - No Penalty'),
        (fake_pen, 'MOPO Penalty'),
        (gym_rewards, 'Real Env.'),
    ]):
        # Note: will encounter issues here where some episodes are shorter than others
        # Would need to change to `np.nanX` methods
        # Will not impact HalfCheetah episodes, as these always have length 1000 (unless an exception is hit)
        cumsum_arr = metric.cumsum(axis=1)
        mean_arr = cumsum_arr.mean(axis=0).flatten()
        min_arr = cumsum_arr.min(axis=0).flatten()
        max_arr = cumsum_arr.max(axis=0).flatten()

        ax.plot(mean_arr, c=cols[i], label=label)
        ax.fill_between(np.arange(n_records), min_arr, max_arr, color=cols[i], alpha=0.5)

    ax.set_title(policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy))

    ax.legend()

def plot_reward_mse(policy_exp, dynamics_exp=None, deterministic_model=True, deterministic_policy=True):
    """ Plot the reward mean squared error (MSE) over the duration of a set of episodes.
    """
    fig, ax = plt.subplots(1, 1, figsize=(20,10))

    if dynamics_exp is None:
        dynamics_exp = get_experiment_details(policy_exp).dynamics_model_exp

    fake_unpen_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'fake', 'unpen_rewards')
    eval_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'eval', 'rewards')

    squared_err = (fake_unpen_rewards-eval_rewards)**2

    mse = squared_err.mean(axis=-1).mean(axis=0)
    mse_std = squared_err.std(axis=-1).mean(axis=0)

    n_records = fake_unpen_rewards.shape[1]

    ax.plot(mse)
    ax.fill_between(np.arange(n_records), mse-2*mse_std, mse+2*mse_std, alpha=0.5)

def plot_visitation_landscape(dynamics_exp, policy_exp):
    """ Use PCA to project the state-action space visited by a collection of episodes into two dimensions.
    The visitations for episodes using both the real and learned environment will be shown to allow comparisons.
    """
    _, ax = plt.subplots(1, 1, figsize=(10,10))

    _, fake_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')))
    _, gym_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'state_action')))

    ax.scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], marker='x', s=10, label='Model')
    ax.scatter(gym_pca_2d_arrs[:,0], gym_pca_2d_arrs[:,1], marker='x', s=10, label='Real Environment')

    ax.legend()

def policy_dynamics_pool_visitation_pca(policy_exp_list_label_set):
    """ Learn a 2D PCA projection matrix from the state-action space visited by a collection of episodes.
    Visitations for episodes using both the real and learned environment will be used.
    The results for episodes generated using multiple different policies will be used.
    """
    policy_exp_lists = [i[0] for i in policy_exp_list_label_set]
    policy_exps = [i for j in policy_exp_lists for i in j]

    sa_arrs = []
    for policy_exp in policy_exps:
        policy_exp_details = get_experiment_details(policy_exp)
        dynamics_exp = policy_exp_details.dynamics_model_exp

        try:
            fake_sa_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action'))
            gym_sa_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'state_action'))
            sa_arrs.append(np.vstack((fake_sa_arrs, gym_sa_arrs)))
        except MetricNotFound:
            continue
    
    sa_arrs = np.vstack(sa_arrs)

    pca_2d = learn_project_arr_2d(sa_arrs)
    _, _, explained_var = project_arr(sa_arrs, pca_2d=pca_2d)

    try:
        assert np.round(pca_2d.explained_variance_ratio_.sum(), 8) == np.round(explained_var, 8)
    except AssertionError as e:
        print(pca_2d.explained_variance_ratio_.sum(), explained_var)
        raise e

    return pca_2d

def policy_dynamics_pool_visitation_2dhist(policy_exp_list_label_set, mode, eval_env, vmin=None, vmax=None, pca_model=None):
    """ Extract and transform transition records from a collection of episodes.
    Transitions for episodes using both the real and learned environment are collected.
    The records are parsed such that the `model_pool_visitation_2dhist` function can be re-used, as if the transitions had been sampled
    from the model pool during policy training.
    """
    results_arr = {}
    policy_exp_lists = [i[0] for i in policy_exp_list_label_set]
    policy_exps = [i for j in policy_exp_lists for i in j]
    for policy_exp in policy_exps:
        policy_exp_details = get_experiment_details(policy_exp)
        dynamics_exp = policy_exp_details.dynamics_model_exp

        try:
            if eval_env == 'fake':
                _, pca_2d_arrs, explained_var = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')), pca_2d=pca_model)

                n_records = len(pca_2d_arrs)

                fake_unpen_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards'))
                fake_reward_pens_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'reward_pens'))
                eval_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'eval', 'rewards'))
                
                reward_arrs = fake_unpen_rewards_arrs - fake_reward_pens_arrs * policy_exp_details.penalty_coeff
                reward_pens_arrs = fake_reward_pens_arrs
                mse_arrs = (fake_unpen_rewards_arrs-eval_rewards_arrs)**2
            elif eval_env == 'gym':
                _, pca_2d_arrs, explained_var = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'state_action')), pca_2d=pca_model)

                n_records = len(pca_2d_arrs)

                reward_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'rewards'))
                reward_pens_arrs = np.zeros((n_records, 1))
                mse_arrs = np.zeros((n_records, 1))
            else:
                raise RuntimeError(f'Do not recognise evaluation environment: {eval_env}')
            
            # Creating a fake pool that only contains rewards and reward penalties
            # This enables the use of the `model_pool_visitation_2dhist` function
            pool = np.hstack((
                np.zeros((n_records, HC_STATE_DIMS+HC_ACTION_DIMS+HC_STATE_DIMS)), reward_arrs, np.zeros((n_records, 2)), reward_pens_arrs
            ))
            results_arr[policy_exp] = PoolArrs(pool=pool, pca_sa_1d=None, pca_sa_2d=pca_2d_arrs, mse_results=mse_arrs, explained_var_2d=explained_var)
        except MetricNotFound:
            results_arr[policy_exp] = None
    
    if mode=='visitation':
        model_pool_visitation_2dhist(policy_exp_list_label_set, vmin=vmin, vmax=vmax, results_arr=results_arr)
    elif mode=='pen-rewards':
        model_pool_pen_rewards_2dhist(policy_exp_list_label_set, vmin=vmin, vmax=vmax, results_arr=results_arr)
    elif mode=='unpen-rewards':
        model_pool_unpen_rewards_2dhist(policy_exp_list_label_set, pen_coeff=policy_exp_details.penalty_coeff, vmin=vmin, vmax=vmax, results_arr=results_arr)
    elif mode=='penalties':
        model_pool_penalties_2dhist(policy_exp_list_label_set, vmin=vmin, vmax=vmax, results_arr=results_arr)
    elif mode=='rmse':
        model_pool_rmse_2dhist(policy_exp_list_label_set, vmin=vmin, vmax=vmax, results_arr=results_arr)
    else:
        raise RuntimeError(f'Cannot handle mode: {mode}')

def plot_reward_landscape(dynamics_exp, policy_exp, deterministic_model=True, deterministic_policy=True):
    """ Use PCA to project the state-action space visited by a collection of episodes into two dimensions, and plot the rewards on a third dimension.
    The states, actions and rewards for episodes using both the real and learned environment will be shown to allow comparisons.
    """
    fig, ax = plt.subplots(1, 2, figsize=(20,10), subplot_kw={"projection": "3d"})

    _, fake_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')))
    _, gym_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'state_action')))
    
    fake_unpen_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards')).flatten()
    eval_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'eval', 'rewards')).flatten()
    gym_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'rewards')).flatten()
    
    # Plotting the same thing twice as it will be shown from two different angles
    for i in range(2):
        ax[i].scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], fake_unpen_rewards_arrs, marker='x', s=10, label='Learned Reward')
        ax[i].scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], eval_rewards_arrs, marker='x', s=10, label='True Return')
        ax[i].scatter(gym_pca_2d_arrs[:,0], gym_pca_2d_arrs[:,1], gym_rewards_arrs, marker='x', s=10, label='Real Env.')
    
    ax[0].view_init(10, 60)
    ax[1].view_init(90, 0)
    
    for i in range(2):
        ax[i].set_xlabel('PCA Dimension 1')
        ax[i].set_ylabel('PCA Dimension 2')
        ax[i].set_zlabel('Reward')
        ax[i].legend()

    fig.suptitle(policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy))

def plot_metric_landscape_comp(dynamics_policy_exps_list, metric):
    """ Use PCA to project the state-action space visited by a collection of episodes into two dimensions, and plot the
    metric specified as an argument to the function call on a third dimension.
    The states, actions and metric values for episodes using both the real and learned environment will be shown to allow comparisons.
    """
    _, ax = plt.subplots(1, 2, figsize=(20,10), subplot_kw={"projection": "3d"})

    for dynamics_exp, policy_exp in dynamics_policy_exps_list:
        _, fake_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')))
        fake_metric_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', metric)).flatten()

        # Plotting the same thing twice as it will be shown from two different angles
        for i in range(2):
            ax[i].scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], fake_metric_arrs, marker='x', s=10, label=f'{dynamics_exp} - {policy_exp}')
        
        ax[0].view_init(10, 60)
        ax[1].view_init(90, 0)

    for i in range(2):
        ax[i].set_xlabel('PCA Dimension 1')
        ax[i].set_ylabel('PCA Dimension 2')
        ax[i].set_zlabel((' '.join(metric.split('_'))).title())
        ax[i].legend()

def get_policy_values(policy_exps, expected_records, deterministic_model=True, deterministic_policy=True, custom_dynamics_exp=None):
    """ Determine the values of a collection of policies (`policy_exps`) by loading and summing rewards data from a collection of episodes.
    The policy value is determined using episodes that were generated using both the real and learned environment.
    Note that `expected_records` is the number of episodes that are expected to have run for each policy.
    This is used to create blank, placeholder records where a metric file cannot be found - other, more sophisticated, methods could be employed.
    """
    xtick_labels = []
    fake_pen_returns_arr = []
    fake_unpen_returns_arr = []
    eval_returns_arr = []
    gym_returns_arr = []

    for policy_exp in policy_exps:
        policy_exp_details = get_experiment_details(policy_exp)

        if custom_dynamics_exp is None:
            dynamics_exp = policy_exp_details.dynamics_model_exp
        else:
            dynamics_exp = custom_dynamics_exp
        mopo_pen_coeff = policy_exp_details.penalty_coeff

        if policy_exp_details.rex:
            exp_label = f'REx $\\beta={policy_exp_details.rex_beta}$\nSeed: {policy_exp_details.seed}'
        else:
            exp_label = f'No REx\nSeed: {policy_exp_details.seed}'
        xtick_labels.append(exp_label)

        try:
            fake_unpen_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'fake', 'unpen_rewards')
            fake_pen = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'fake', 'reward_pens')
            fake_pen_rewards = fake_unpen_rewards - fake_pen*mopo_pen_coeff
            
            eval_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'eval', 'rewards')
            gym_rewards = retrieve_metric(dynamics_exp, policy_exp, deterministic_model, deterministic_policy, 'gym', 'rewards')
        except MetricNotFound:
            # If reward values are not found for a policy, return placeholder NaN values instead.
            # An error will be printed by the `retrieve_metric` method indicating that no metrics could be found.
            print(f'Metric not found for: {policy_exp}')
            fake_pen_returns_arr.append(np.ones(expected_records)*np.nan)
            fake_unpen_returns_arr.append(np.ones(expected_records)*np.nan)
            eval_returns_arr.append(np.ones(expected_records)*np.nan)
            gym_returns_arr.append(np.ones(expected_records)*np.nan)
        else:
            # Use the `np.nanX` methods as some episodes may be shorter than others, and `np.NaN` was used as a filler
            # to create arrays of the same length.
            # Should not generally be necessary in the case of HalfCheetah episodes, as these are always the same length.
            # However, there can be cases where an action is permissible in the learned environment, but throws an error 
            # in the MuJoCo environment. This can result in shorter episodes using the real environment model, even for
            # the HalfCheetah.
            fake_pen_returns_arr.append(np.nansum(fake_pen_rewards, axis=1).flatten())
            fake_unpen_returns_arr.append(np.nansum(fake_unpen_rewards, axis=1).flatten())
            eval_returns_arr.append(np.nansum(eval_rewards, axis=1).flatten())
            gym_returns_arr.append(np.nansum(gym_rewards, axis=1).flatten())

    # Stack the results for all the policies
    fake_pen_returns_arr = np.vstack(fake_pen_returns_arr)
    fake_unpen_returns_arr = np.vstack(fake_unpen_returns_arr)
    eval_returns_arr = np.vstack(eval_returns_arr)
    gym_returns_arr = np.vstack(gym_returns_arr)

    return fake_pen_returns_arr, fake_unpen_returns_arr, eval_returns_arr, gym_returns_arr, xtick_labels

def plot_policy_values(policy_exps, expected_records, deterministic_model=True, deterministic_policy=True, show_penalised=True):
    """ Plot the values of a collection of policies by loading, summing and visualising rewards data from a collection of episodes.

    NOTE: If the return is `-inf` then this will not be plotted - it will appear as a blank; watch out for this!!
    """
    _, ax = plt.subplots(1, 1, figsize=(20,10))

    # Retrieve the policy values
    fake_pen_returns_arr, fake_unpen_returns_arr, eval_returns_arr, gym_returns_arr, xtick_labels = get_policy_values(policy_exps, expected_records, deterministic_model, deterministic_policy)

    # For each policy, determine the mean and standard deviation of the returns across however many epsiodes were created.
    # If episodes metrics were not identified for a given policy, a placeholder array of `np.NaN` values will be present.
    # The `np.nanX` methods are used, however (except in cases where no metrics were identified for a policy) it shouldn't be
    # the case that we have an `np.NaN` return.
    fake_pen_returns_mean, fake_pen_returns_std = np.nanmean(fake_pen_returns_arr, axis=1), np.nanstd(fake_pen_returns_arr, axis=1)
    fake_unpen_returns_mean, fake_unpen_returns_std = np.nanmean(fake_unpen_returns_arr, axis=1), np.nanstd(fake_unpen_returns_arr, axis=1)
    eval_returns_mean, eval_returns_std = np.nanmean(eval_returns_arr, axis=1), np.nanstd(eval_returns_arr, axis=1)
    gym_returns_mean, gym_returns_std = np.nanmean(gym_returns_arr, axis=1), np.nanstd(gym_returns_arr, axis=1)

    if show_penalised:
        ax.errorbar(np.arange(len(policy_exps)), fake_pen_returns_mean, fake_pen_returns_std, ls='', marker='x', label='Learned Returns - Penalised')
    
    ax.errorbar(np.arange(len(policy_exps)), fake_unpen_returns_mean, fake_unpen_returns_std, ls='', marker='x', label='Learned Returns - Unpenalised')
    ax.errorbar(np.arange(len(policy_exps)), eval_returns_mean, eval_returns_std, ls='', marker='x', label='Evaluation Returns')
    ax.errorbar(np.arange(len(policy_exps)), gym_returns_mean, gym_returns_std, ls='', marker='x', label='Gym Returns')

    # ax.set_title(policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy))
    
    ax.set_xlabel('Experiment')
    ax.set_ylabel('Average Return')
    ax.set_xticks(np.arange(len(policy_exps)))
    ax.set_xticklabels(xtick_labels)

    ax.legend()

def plot_dynamics_group_values(dynamics_exp_label_groups, policy_exps, expected_records, deterministic_model=True, deterministic_policy=True, show_penalised=True, ymin=None, ymax=None, save_path=None,
    exclude_degen_returns=False, degen_return_threshold=1e6
):
    """ We can determine the value of a policy using any learned dynamics model. `dynamics_exp_label_groups` will contain a list of dynamics models and a label indiciating the dataset and hyperparameters
    used to train the models. That is, each model will have been trained using the same data and settings, but a different random seed. `policy_exps` will be a list of policies that have been used to
    generate a collection of episodes, against the environment models in `dynamics_exp_label_groups`. Typically, these policies will have been trained using the same data and settings (including
    the same dynamics model), but different random seeds.

    The purpose of this function is to plot the mean and standard deviation of the value of the policies in each set of learned dynamics models. That is, there will be a value with error bars for each
    set of dynamics experiments. Each entry will relate to the value of the same set of policies: `policy_exps`. If there are three policies in `policy_exps` and each set of dynamics experiments in
    `dynamics_exp_label_groups` contains three dynamics models then there will therefore be nine policy/dynamics model combinations. The returns for the number of epsiodes (`expected_records`)
    generated using each policy/dynamics model combination will be loaded. The average and standard deviation over all these returns will be calculated and plotted as a single entry in the graph.

    NOTE: If the return is `-inf` then this will not be plotted - it will appear as a blank; watch out for this!!
    """
    plt.rc('font', size=22)
    fig, ax = plt.subplots(1, 1, figsize=(10,10))

    xtick_labels = []
    grouped_fake_pen_returns = np.zeros((len(dynamics_exp_label_groups),2))
    grouped_fake_unpen_returns = np.zeros((len(dynamics_exp_label_groups),2))
    grouped_eval_returns = np.zeros((len(dynamics_exp_label_groups),2))
    grouped_gym_returns = np.zeros((len(dynamics_exp_label_groups),2))

    # Loop over each of the sets of dynamics models
    for i, (dynamics_exps, label) in enumerate(dynamics_exp_label_groups):
        xtick_labels.append(label)

        fake_pen_returns_arr = np.zeros((len(dynamics_exps), len(policy_exps), expected_records))
        fake_unpen_returns_arr = np.zeros((len(dynamics_exps), len(policy_exps), expected_records))
        eval_returns_arr = np.zeros((len(dynamics_exps), len(policy_exps), expected_records))
        gym_returns_arr = np.zeros((len(dynamics_exps), len(policy_exps), expected_records))

        for j, dynamics_exp in enumerate(dynamics_exps):
            fake_pen_returns_arr[j,:,:], fake_unpen_returns_arr[j,:,:], eval_returns_arr[j,:,:], gym_returns_arr[j,:,:], _ = get_policy_values(policy_exps, expected_records, deterministic_model, deterministic_policy, custom_dynamics_exp=dynamics_exp)

        # Exclude returns that exceed the `degen_return_threshold` if `exclude_degen_returns` is True
        # This is valid to do so long as the presence of degenerate episodes is also highlighted.
        degen_return_mask = np.abs(fake_pen_returns_arr) > degen_return_threshold
        if not exclude_degen_returns:
            degen_return_mask = np.zeros_like(degen_return_mask)
        
        # Apply the mask that will exclude degenerate returns - by default this will not exclude any records
        # NOTE: The mask is applied to returns from the eval and real environments too - we effectively completely ignore that(/those) episode(s)
        fake_pen_returns_arr = fake_pen_returns_arr[~degen_return_mask]
        fake_unpen_returns_arr = fake_unpen_returns_arr[~degen_return_mask]
        eval_returns_arr = eval_returns_arr[~degen_return_mask]
        gym_returns_arr = gym_returns_arr[~degen_return_mask]

        grouped_fake_pen_returns[i,0], grouped_fake_pen_returns[i,1] = np.nanmean(fake_pen_returns_arr), np.nanstd(fake_pen_returns_arr)
        grouped_fake_unpen_returns[i,0], grouped_fake_unpen_returns[i,1] = np.nanmean(fake_unpen_returns_arr), np.nanstd(fake_unpen_returns_arr)
        grouped_eval_returns[i,0], grouped_eval_returns[i,1] = np.nanmean(eval_returns_arr), np.nanstd(eval_returns_arr)
        grouped_gym_returns[i,0], grouped_gym_returns[i,1] = np.nanmean(gym_returns_arr), np.nanstd(gym_returns_arr)

    # sc is the starting colour index, which defaults to 0, but is set to 1 if we are showing the penalised rewards
    sc = 0
    if show_penalised:
        sc = 1
        ax.errorbar(np.arange(len(dynamics_exp_label_groups)), grouped_fake_pen_returns[:,0], grouped_fake_pen_returns[:,1], color=cols[0], ls='', marker='x', capsize=10, label='Learned Dynamics - Penalised')

    ax.errorbar(np.arange(len(dynamics_exp_label_groups)), grouped_fake_unpen_returns[:,0], grouped_fake_unpen_returns[:,1], color=cols[sc], ls='', marker='x', capsize=10, label='Dynamics Model')
    # ax.errorbar(np.arange(len(dynamics_exp_label_groups)), grouped_eval_returns[:,0], grouped_eval_returns[:,1], color=cols[sc+1], ls='', marker='x', capsize=10, label='Eval. Environment')

    # Plot a single line for the policy value in the real environment, and indicate ± one standard deviation.
    # This uses the averages of the mean and standard deviation values obtained for each collection of dynamics models.
    # It should be remembered that all of these values relate to the value of the policy in the real environment - regardless of the dynamics model set.
    # NOTE: It is important to be aware of the initial state distribution that was being used.
    # NOTE: Taking the mean standard devation is not ideal - could follow a similar approach to `plot_returns_comparison_pol_dep_groups`
    ax.axhline(grouped_gym_returns[:,0].mean(), color=cols[sc+2], label='Real Environment')
    ax.axhline(grouped_gym_returns[:,0].mean()-grouped_gym_returns[:,1].mean(), color=cols[sc+2], ls='--')
    ax.axhline(grouped_gym_returns[:,0].mean()+grouped_gym_returns[:,1].mean(), color=cols[sc+2], ls='--')

    # The below would plot the environment returns for each dynamics model set individually - which would be appropriate to consider when the initial state distribution
    # differs across the dynamics model sets. This would be the case if starting locations were being drawn from the dataset used to train the dynamics models.
    # ax.errorbar(np.arange(len(dynamics_exp_label_groups)), grouped_gym_returns[:,0], grouped_gym_returns[:,1], color=cols[sc+2], ls='', marker='x', label='Real Environment')

    # ax.set_title(policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy))
    ax.set_xlabel('Dynamics Model')
    ax.set_ylabel('Average Return')
    ax.set_ylim(ymin, ymax)
    ax.set_xticks(np.arange(len(dynamics_exp_label_groups)))
    ax.set_xticklabels(xtick_labels, rotation=60)

    ax.legend()

    if save_path is not None:
        fig.savefig(os.path.join(FIG_DIR, save_path), pad_inches=0.2, bbox_inches='tight')

def plot_policy_group_values(
    policy_exp_label_groups, expected_records, deterministic_model=True, deterministic_policy=True, show_penalised=True, loc='upper left', y_min=None, y_max=None, save_path=None, x_label=None, x_tick_rot=45, fig_size=(10,10),
    exclude_degen_returns=False, degen_return_threshold=1e6
    ):
    """ Plot the values of a collection of policies by loading, summing and visualising rewards data from a collection of episodes. This is a similar function to `plot_policy_values` although enables the results for different
    seeds to be grouped together (i.e., take a means and standard deviation over the group of policies trained using dynamics models that were trained on the same datasets and with the same hyperparameters).
    """
    plt.rc('font', size=22)
    fig, ax = plt.subplots(1, 1, figsize=fig_size)

    xtick_labels = []
    grouped_fake_pen_returns = np.zeros((len(policy_exp_label_groups),2))
    grouped_fake_unpen_returns = np.zeros((len(policy_exp_label_groups),2))
    grouped_eval_returns = np.zeros((len(policy_exp_label_groups),2))
    grouped_gym_returns = np.zeros((len(policy_exp_label_groups),2))
    grouped_inf_count = np.zeros((len(policy_exp_label_groups),1))
    grouped_return_diff = np.zeros((len(policy_exp_label_groups),2))

    for i, (policy_exps, label) in enumerate(policy_exp_label_groups):
        xtick_labels.append(label)
        fake_pen_returns_arr, fake_unpen_returns_arr, eval_returns_arr, gym_returns_arr, _ = get_policy_values(policy_exps, expected_records, deterministic_model, deterministic_policy)

        # Determine how many returns exceed the `degen_return_threshold`.
        # A flag passed to the function dictates whether these are excluded from plotting. This is valid to do so long as the presence of degenerate episodes is also highlighted.
        # Whether the flag is set to True or not, we count the instances where this occurs and include it in the summary statistics printed alongside the plot
        degen_return_mask = np.abs(fake_pen_returns_arr) > degen_return_threshold
        grouped_inf_count[i,0] = np.sum(degen_return_mask)
        if not exclude_degen_returns:
            degen_return_mask = np.zeros_like(degen_return_mask)
        
        # Apply the mask that will exclude degenerate returns - by default this will not exclude any records
        # NOTE: The mask is applied to returns from the eval and real environments too - we effectively completely ignore that(/those) episode(s)
        fake_pen_returns_arr = fake_pen_returns_arr[~degen_return_mask]
        fake_unpen_returns_arr = fake_unpen_returns_arr[~degen_return_mask]
        eval_returns_arr = eval_returns_arr[~degen_return_mask]
        gym_returns_arr = gym_returns_arr[~degen_return_mask]

        grouped_fake_pen_returns[i,0], grouped_fake_pen_returns[i,1] = np.nanmean(fake_pen_returns_arr), np.nanstd(fake_pen_returns_arr)
        grouped_fake_unpen_returns[i,0], grouped_fake_unpen_returns[i,1] = np.nanmean(fake_unpen_returns_arr), np.nanstd(fake_unpen_returns_arr)
        grouped_eval_returns[i,0], grouped_eval_returns[i,1] = np.nanmean(eval_returns_arr), np.nanstd(eval_returns_arr)
        grouped_gym_returns[i,0], grouped_gym_returns[i,1] = np.nanmean(gym_returns_arr), np.nanstd(gym_returns_arr)
        grouped_return_diff[i,0], grouped_return_diff[i,1] = np.nanmean(fake_unpen_returns_arr-gym_returns_arr), np.nanstd(fake_unpen_returns_arr-gym_returns_arr)

    if show_penalised:
        ax.errorbar(np.arange(len(policy_exp_label_groups)), grouped_fake_pen_returns[:,0], grouped_fake_pen_returns[:,1], ls='', marker='x', markersize=10, capsize=10, label='Learned Dynamics - Penalised')
    ax.errorbar(np.arange(len(policy_exp_label_groups)), grouped_fake_unpen_returns[:,0], grouped_fake_unpen_returns[:,1], ls='', marker='x', markersize=10, capsize=10, label='Learned Dynamics, Learned Rewards')
    ax.errorbar(np.arange(len(policy_exp_label_groups)), grouped_eval_returns[:,0], grouped_eval_returns[:,1], ls='', marker='x', markersize=10, capsize=10, label='Learned Dynamics, Real Rewards')
    ax.errorbar(np.arange(len(policy_exp_label_groups)), grouped_gym_returns[:,0], grouped_gym_returns[:,1], ls='', marker='x', markersize=10, capsize=10, label='Real Dynamics, Real Rewards')

    # ax.set_title(policy_dynamics_plot_title(dynamics_exp, policy_exp, deterministic_model, deterministic_policy))
    ax.set_xlabel(x_label or 'REx and MOPO Penalty Coefficients')
    ax.set_ylabel('Average Return')
    ax.set_xticks(np.arange(len(policy_exp_label_groups)))
    ax.set_xticklabels(xtick_labels, rotation=x_tick_rot)

    ax.set_ylim(y_min, y_max)
    
    # Print summary statistics
    print('Labels', xtick_labels)
    print('Fake', list(grouped_fake_unpen_returns[:,0]), list(grouped_fake_unpen_returns[:,1]))
    print('Eval', list(grouped_eval_returns[:,0]), list(grouped_eval_returns[:,1]))
    print('Real', list(grouped_gym_returns[:,0]), list(grouped_gym_returns[:,1]))
    print('Degenerate Reward Episode Count', list(grouped_inf_count[:,0]))
    print('Diff', list(grouped_return_diff[:,0]), list(grouped_return_diff[:,1]))

    ax.legend(loc=loc)

    if save_path is not None:
        fig.savefig(os.path.join(FIG_DIR, save_path), pad_inches=0.2, bbox_inches='tight')

def get_dynamics_pol_scores(dynamics_exps, policy_exps, exclude_degen_returns=False, degen_return_threshold=1e6):
    """ Return the mean and standard deviation of the returns of episodes run using all the pairwise combinations of the environment models and policies
    in the passed lists of `dynamics_exps` and `policy_exps`.

    NOTE: Unlike some earlier functions, `MetricNotFound` exceptions raised by `retrieve_metric` are not caught.
    """
    dynamics_exps_labels = []
    policy_exps_labels = []

    fake_returns = np.zeros((len(dynamics_exps), len(policy_exps), 2))
    eval_returns = np.zeros((len(dynamics_exps), len(policy_exps), 2))
    gym_returns = np.zeros((len(dynamics_exps), len(policy_exps), 2))

    for i, dynamics_exp in enumerate(dynamics_exps):
        dynamics_exp_details = get_experiment_details(dynamics_exp)
        # if dynamics_exp_details.rex:
        #     dynamics_exps_labels.append(f'{dynamics_exp} - REx: {dynamics_exp_details.rex}\nREx Beta: {dynamics_exp_details.rex_beta}\nSeed: {dynamics_exp_details.seed}')
        # else:
        #     dynamics_exps_labels.append(f'{dynamics_exp} - REx: {dynamics_exp_details.rex}\nSeed: {dynamics_exp_details.seed}')
        dynamics_exps_labels.append(f'{dynamics_exp_details.dataset}\nSeed: {dynamics_exp_details.seed}')
        
        for j, policy_exp in enumerate(policy_exps):
            if i == 0:
                policy_exp_details = get_experiment_details(policy_exp)
                policy_dynamics_exp_details = get_experiment_details(policy_exp_details.dynamics_model_exp) if policy_exp_details.dynamics_model_exp else policy_exp_details
                # if policy_dynamics_exp_details.rex:
                #     policy_exps_labels.append(f'{policy_dynamics_exp_details.name} - REx: {policy_dynamics_exp_details.rex}\nREx Beta: {policy_dynamics_exp_details.rex_beta}\nSeed: {dynamics_exp_details.seed}')
                # else:
                #     policy_exps_labels.append(f'{policy_dynamics_exp_details.name} - REx: {policy_dynamics_exp_details.rex}\nSeed: {dynamics_exp_details.seed}')
                policy_exps_labels.append(f'{policy_dynamics_exp_details.dataset}\nSeed: {policy_dynamics_exp_details.seed}')

            fake_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards'), axis=1).flatten()

            # If `exclude_degen_returns` is True create a mask for those return values that exceed the `degen_return_threshold`.
            # This is valid to do so long as the presence of degenerate episodes is also highlighted.
            if exclude_degen_returns:
                degen_return_mask = np.abs(fake_returns_arr) > degen_return_threshold
            else:
                degen_return_mask = np.zeros_like(fake_returns_arr)
            
            # Apply the mask that will exclude degenerate returns - by default this will not exclude any records
            # NOTE: The mask is applied to returns from the eval and real environments too - we effectively completely ignore that(/those) episode(s)
            fake_returns_arr = np.where(degen_return_mask, np.NaN, fake_returns_arr)
            fake_returns[i,j,0], fake_returns[i,j,1] = np.nanmean(fake_returns_arr), np.nanstd(fake_returns_arr)

            eval_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'eval', 'rewards'), axis=1).flatten()
            eval_returns_arr = np.where(degen_return_mask, np.NaN, eval_returns_arr)
            eval_returns[i,j,0], eval_returns[i,j,1] = np.nanmean(eval_returns_arr), np.nanstd(eval_returns_arr)

            gym_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'rewards'), axis=1).flatten()
            gym_returns_arr = np.where(degen_return_mask, np.NaN, gym_returns_arr)
            gym_returns[i,j,0], gym_returns[i,j,1] = np.nanmean(gym_returns_arr), np.nanstd(gym_returns_arr)

    return fake_returns, eval_returns, gym_returns, dynamics_exps_labels, policy_exps_labels

def plot_returns_comparison(dynamics_exps, policy_exps, exclude_degen_returns=False, degen_return_threshold=1e6, font_size=22):
    """ Plot the matrices obtained from the `get_dynamics_pol_scores` function.
    """
    _, ax = plt.subplots(1, 3, figsize=(33,10))
    plt.rc('font', size=font_size)

    fake_returns, eval_returns, gym_returns, dynamics_exps_labels, policy_exps_labels = get_dynamics_pol_scores(
        dynamics_exps, policy_exps, exclude_degen_returns=exclude_degen_returns, degen_return_threshold=degen_return_threshold
    )
    
    for i, (res_arr, title) in enumerate([
        (fake_returns, 'Dynamics Model'),
        (eval_returns, 'True Return'),
        (gym_returns, 'Real Env.')
    ]):
        ax[i].matshow(res_arr[:,:,0], cmap='viridis')
        ax[i].set_xticks(range(len(policy_exps)))
        ax[i].set_yticks(range(len(dynamics_exps)))
        ax[i].set_xticklabels(policy_exps_labels, rotation=45)

        if title != 'Real Env.':
            ax[i].set_yticklabels(dynamics_exps_labels, rotation=45)
        else:
            ax[i].set_yticklabels([])
        ax[i].set_title(title)

        for (j,k), z in np.ndenumerate(res_arr[:,:,0]):
            if z != 0:
                ax[i].text(k, j, '{:.2f}\n±{:.2f}'.format(res_arr[j,k,0], res_arr[j,k,1]), ha="center", va="center", color='w' if z < 1000 else 'k')

    for i in range(3):
        ax[i].set_xlabel('Policy Training Dataset')
        ax[i].set_ylabel('Dynamics Training Dataset')

def get_dynamics_overall_metrics(dynamics_exps, policy_exps, exclude_degen_returns=False, degen_return_threshold=1e6):
    """ Return the mean and standard deviation of the returns of episodes run using all pairwise combinations of the environment models and policies
    in the passed lists of `dynamics_exps` and `policy_exps`.

    This function is distinct from `get_dynamics_pol_scores`, which cannot be reused here. The purpose of `get_dynamics_pol_scores` is to return the
    mean and standard deviation of the returns of episodes run using all pairwise combinations of the environment models and policies in the passed 
    lists of `dynamics_exps` and `policy_exps`. It returns a matrix of dimension: 'num dynamics model' X 'num policies'.

    However, in this function we combine all of the returns for all pairwise combinations of the environment models and policies in the passed lists of 
    `dynamics_exps` and `policy_exps` and then take the mean and standard deviation of this. While taking an average of the matrix of means returned by
    the `get_dynamics_pol_scores` function would yield the same values as the below code, the mean of the matrix of standard deviations is different
    from taking the standard deviation over all returns.

    NOTE: Unlike some earlier functions, `MetricNotFound` exceptions raised by `retrieve_metric` are not caught.
    """
    fake_returns = []
    eval_returns = []
    gym_returns = []

    for dynamics_exp in dynamics_exps:
        for policy_exp in policy_exps:
            fake_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards'), axis=1).flatten()

            # If `exclude_degen_returns` create a mask for those return values that exceed the `degen_return_threshold`.
            # This is valid to do so long as the presence of degenerate episodes is also highlighted.
            if exclude_degen_returns:
                degen_return_mask = np.abs(fake_returns_arr) > degen_return_threshold
            else:
                degen_return_mask = np.zeros_like(fake_returns_arr)
            fake_returns.append(np.where(degen_return_mask, np.NaN, fake_returns_arr))

            # NOTE: The mask is applied to returns from the eval and real environments too - we effectively completely ignore that(/those) episode(s)
            gym_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'gym', 'rewards'), axis=1).flatten()
            gym_returns.append(np.where(degen_return_mask, np.NaN, gym_returns_arr))

            eval_returns_arr = np.nansum(retrieve_metric(dynamics_exp, policy_exp, True, True, 'eval', 'rewards'), axis=1).flatten()
            eval_returns.append(np.where(degen_return_mask, np.NaN, eval_returns_arr))

    # Uncomment these to work around the desirable issue mentioned below
    # fake_returns = np.hstack(fake_returns)
    # eval_returns = np.hstack(eval_returns)
    # gym_returns = np.hstack(gym_returns)

    # An error will be thrown if the arrays in `fake_returns` are not all the same length
    # This is desirable - it means that a different number of episodes was genereated for at least one of the dynamics model/policy pairs
    fake_returns_stats = (np.nanmean(fake_returns), np.nanstd(fake_returns))
    eval_returns_stats = (np.nanmean(eval_returns), np.nanstd(eval_returns))
    gym_returns_stats = (np.nanmean(gym_returns), np.nanstd(gym_returns))

    return fake_returns_stats, eval_returns_stats, gym_returns_stats

def plot_returns_comparison_pol_dep_groups(
    dynamics_exp_groups_labels, policy_exp_groups_labels, w_lim=1000, save_path=None, fig_size=(25.5,10),exclude_degen_returns=False, degen_return_threshold=1e6
):
    """ Plot a heatmap of the returns for collections of learned dynamics models and policies. Seperate plots are shown for returns calculated using the real
    environment and learned environment. The returns for an "evaluation" environment are also shown, which corresponds to using the learned dynamics model for
    transitions, but the real reward function.
    """
    plt.rc('font', size=24)
    fig = plt.figure(figsize=fig_size, tight_layout=True)

    spec = gridspec.GridSpec(ncols=15, nrows=1, figure=fig)
    ax1 = fig.add_subplot(spec[0,1:7])
    ax2 = fig.add_subplot(spec[0,7:13])
    ax3 = fig.add_subplot(spec[0,13:15])
    
    n_dynamics_groups = len(dynamics_exp_groups_labels)
    n_policy_groups = len(policy_exp_groups_labels)

    # Get the returns for the learned and evaluation environments
    fake_returns = np.zeros((n_policy_groups, n_dynamics_groups, 2))
    eval_returns = np.zeros((n_policy_groups, n_dynamics_groups, 2))
    for i, (dynamic_exp_group, _) in enumerate(dynamics_exp_groups_labels):
        for j, (policy_exp_group, _) in enumerate(policy_exp_groups_labels):
            group_fake_return_stats, group_eval_return_stats, _ = get_dynamics_overall_metrics(
                dynamic_exp_group, policy_exp_group, exclude_degen_returns=exclude_degen_returns, degen_return_threshold=degen_return_threshold
            )
            fake_returns[j,i,0], fake_returns[j,i,1] = group_fake_return_stats
            eval_returns[j,i,0], eval_returns[j,i,1] = group_eval_return_stats

    # For the real environment we want to get the returns for each policy across all dynamics models, given that the real environment was used
    # for evaluation each time.
    all_dynamics_exps = [item for sublist in [i[0] for i in dynamics_exp_groups_labels] for item in sublist]
    gym_returns = np.zeros((n_policy_groups, 1, 2))
    for i, (policy_exp_group, _) in enumerate(policy_exp_groups_labels):
        _, _, group_gym_return_stats = get_dynamics_overall_metrics(
            all_dynamics_exps, policy_exp_group, exclude_degen_returns=exclude_degen_returns, degen_return_threshold=degen_return_threshold
        )
        gym_returns[i,0,0], gym_returns[i,0,1] = group_gym_return_stats
    
    for i, (ax, res_arr, title) in enumerate([
        (ax1, fake_returns, 'Learned Transitions\nLearned Rewards'),
        (ax2, eval_returns, 'Learned Transitions\nReal Rewards'),
        (ax3, gym_returns, 'Real Transitions\nReal Rewards')
    ]):
        ax.matshow(res_arr[:,:,0], cmap='viridis')

        ax.tick_params(axis='both', which='major', labelbottom = True, bottom=False, top = False, labeltop=False)
        ax.set_yticks(range(n_policy_groups))
        ax.set_yticklabels([i[1] for i in policy_exp_groups_labels], rotation=0)
        ax.set_ylabel('Policy Demonstrator Steps')

        if i != 2:
            ax.set_xlabel('Dynamics Demonstrator Steps')
            ax.set_xticks(range(n_dynamics_groups))
            ax.set_xticklabels([i[1] for i in dynamics_exp_groups_labels], rotation=0)
        else:
            ax.set_xticks(range(1))
            ax.set_xticklabels([' '], rotation=0)
        ax.set_title(title)

        for (j,k), z in np.ndenumerate(res_arr[:,:,0]):
            if z != 0 and z != np.nan:
                ax.text(k, j, '{:.0f}\n±{:.0f}'.format(res_arr[j,k,0], res_arr[j,k,1]), ha="center", va="center", color='w' if z < w_lim else 'k')

    if save_path is not None:
        fig.savefig(os.path.join(FIG_DIR, save_path), pad_inches=0.2, bbox_inches='tight')
    
def plot_returns_comparison_pol_dep_groups_fake_env(
    dynamics_exp_groups_labels, policy_exp_groups_labels, w_lim=1000, save_path=None, fig_size=(10,10), exclude_degen_returns=False, degen_return_threshold=1e6
):
    """ Effectively the same as `plot_returns_comparison_pol_dep_groups`, but only shows the results for the fake environment.
    """
    plt.rc('font', size=24)
    fig, ax = plt.subplots(1, 1, figsize=fig_size)
    
    n_dynamics_groups = len(dynamics_exp_groups_labels)
    n_policy_groups = len(policy_exp_groups_labels)

    # Get the returns for each group of dynamics models and policies
    fake_returns = np.zeros((n_policy_groups, n_dynamics_groups, 2))
    for i, (dynamic_exp_group, _) in enumerate(dynamics_exp_groups_labels):
        for j, (policy_exp_group, _) in enumerate(policy_exp_groups_labels):
            group_fake_return_stats, _, _ = get_dynamics_overall_metrics(
                dynamic_exp_group, policy_exp_group, exclude_degen_returns=exclude_degen_returns, degen_return_threshold=degen_return_threshold
            )
            fake_returns[j,i,0], fake_returns[j,i,1] = group_fake_return_stats

    ax.matshow(fake_returns[:,:,0], cmap='viridis')

    ax.tick_params(axis='both', which='major', labelbottom = True, bottom=False, top = False, labeltop=False)

    ax.set_xticks(range(n_dynamics_groups))
    ax.set_xticklabels([i[1] for i in dynamics_exp_groups_labels], rotation=0)
    ax.set_xlabel('Dynamics Demonstrator Steps')

    ax.set_yticks(range(n_policy_groups))
    ax.set_yticklabels([i[1] for i in policy_exp_groups_labels], rotation=0)
    ax.set_ylabel('Policy Demonstrator Steps')

    for (j,k), z in np.ndenumerate(fake_returns[:,:,0]):
        if z != 0 and z != np.nan:
            ax.text(k, j, '{:.0f}\n±{:.0f}'.format(fake_returns[j,k,0], fake_returns[j,k,1]), ha="center", va="center", color='w' if z < w_lim else 'k', fontsize=20)

    if save_path is not None:
        fig.savefig(os.path.join(FIG_DIR, save_path), pad_inches=0.2, bbox_inches='tight')

def plot_pen_reward_landscape_comp(dynamics_policy_exps_list, pen_coeff=1.0):
    _, ax = plt.subplots(1, 2, figsize=(20,10), subplot_kw={"projection": "3d"})

    for dynamics_exp, policy_exp in dynamics_policy_exps_list:
        _, fake_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')))
        
        fake_unpen_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards')).flatten()
        fake_reward_pens_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'reward_pens')).flatten()

        fake_pen_reward_arrs = fake_unpen_rewards_arrs - (float(pen_coeff) * fake_reward_pens_arrs)

        for i in range(2):
            ax[i].scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], fake_pen_reward_arrs, marker='x', s=10, label=f'{dynamics_exp} - {policy_exp}')
        
        ax[0].view_init(10, 60)
        ax[1].view_init(90, 0)

    for i in range(2):
        ax[i].set_xlabel('PCA Dimension 1')
        ax[i].set_ylabel('PCA Dimension 2')
        ax[i].set_zlabel('Pen Reward')
        ax[i].legend()

def plot_reward_error_landscape_comp(dynamics_policy_exps_list, training_dataset=False):
    _, ax = plt.subplots(1, 2, figsize=(20,10), subplot_kw={"projection": "3d"})

    for dynamics_exp, policy_exp in dynamics_policy_exps_list:
        _, fake_pca_2d_arrs = project_arr(np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'state_action')))

        fake_unpen_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'fake', 'unpen_rewards')).flatten()
        eval_rewards_arrs = np.vstack(retrieve_metric(dynamics_exp, policy_exp, True, True, 'eval', 'rewards')).flatten()
        squared_err_arrs = (fake_unpen_rewards_arrs-eval_rewards_arrs)**2

        for i in range(2):
            ax[i].scatter(fake_pca_2d_arrs[:,0], fake_pca_2d_arrs[:,1], squared_err_arrs, marker='x', s=10, label=f'{dynamics_exp} - {policy_exp}')
        
        ax[0].view_init(10, 60)
        ax[1].view_init(90, 0)
    
    if training_dataset:
        dynamics_exp_details = get_experiment_details(dynamics_exp)
        _, data_pca_2d_arr = project_arr(np.load(os.path.join(DATA_DIR, f'{dynamics_exp_details.dataset}.npy'))[:,:HC_STATE_DIMS+HC_ACTION_DIMS])
        for i in range(2):
            ax[i].scatter(data_pca_2d_arr[:,0], data_pca_2d_arr[:,1], -0.1*np.ones_like(data_pca_2d_arr[:,1]), marker='+', s=10, color='lightgray', label=f'Training Data')

    for i in range(2):
        ax[i].set_xlabel('PCA Dimension 1')
        ax[i].set_ylabel('PCA Dimension 2')
        ax[i].set_zlabel('Reward Squared Error')
        ax[i].legend()
